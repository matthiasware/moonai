{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b0bc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c3e782",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "#\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "#\n",
    "from tqdm import tqdm\n",
    "#\n",
    "from problems import get_problems, filter_problems\n",
    "from metrics import *\n",
    "from utils import coords_map, grade_maps, get_board_setup, draw_moves, plot_mat, count_parameters, x_coords, y_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2325afc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "P_ROOT = Path(\"./data\")\n",
    "P_PROB = P_ROOT / \"problems\"\n",
    "P_BOARD = P_ROOT / \"boards/moonboard.png\"\n",
    "P_HOLDS = P_ROOT / \"boards/holds\"\n",
    "setup_year = 2017\n",
    "setup_angle = 45\n",
    "#\n",
    "minlen = 4\n",
    "maxlen = 14\n",
    "#\n",
    "grade_names = [\"6A+\", \"6B\", \"6B+\", \"6C\", \"6C+\", \"7A\", \"7A+\", \"7B\", \"7B+\", \"7C\", \"7C+\"]\n",
    "repsge = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c315deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = get_problems(P_PROB, setup_year, setup_angle)\n",
    "print(len(probs))\n",
    "\n",
    "probs = filter_problems(probs, repsge, grade_names, minlen, maxlen)\n",
    "print(len(probs))\n",
    "\n",
    "grade_to_num, num_to_grade = grade_maps(grade_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6f91e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "grades, nprobs = zip(*Counter(sorted([grade_to_num[p[\"grade\"]] for p in probs])).items())\n",
    "nprobs = np.array(nprobs)\n",
    "grade_freqs =  nprobs / nprobs.sum()\n",
    "class_weights = nprobs.sum() / nprobs\n",
    "\n",
    "fig, axes = plt.subplots(1, 1, figsize=(16, 8))\n",
    "plt.bar(grades, nprobs)\n",
    "#plt.yscale(\"log\")\n",
    "plt.ylabel(\"#probs\")\n",
    "plt.xlabel(\"#grades\")\n",
    "plt.xticks(grades, [num_to_grade[n] for n in grades])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd63d2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords_to_num = coords_map\n",
    "num_to_coords = {v:k for k,v in coords_to_num.items()}\n",
    "#\n",
    "spec_to_num = {\n",
    "    \"<CLS>\": max(num_to_coords.keys()) + 1,\n",
    "    \"<PAD>\": max(num_to_coords.keys()) + 2\n",
    "}\n",
    "num_to_spec = {v:k for k,v in spec_to_num.items()}\n",
    "#\n",
    "tok_to_num = {**spec_to_num, **coords_to_num}\n",
    "num_to_tok = {v:k for k,v in tok_to_num.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4781ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def specify(sent, pad_to):\n",
    "    \"\"\"\n",
    "        Add special tokens and pad\n",
    "    \"\"\"\n",
    "    assert len(sent) > 1, \"Sentency empty\"\n",
    "    assert len(sent) < pad_to, \"Sentences too long!, Increase padding!\"\n",
    "    return [\"<CLS>\"] + sent + [\"<PAD>\"] * (pad_to - len(sent))\n",
    "\n",
    "def maskify(sent):\n",
    "    \"\"\"\n",
    "        Create mask\n",
    "    \"\"\"\n",
    "    return [False if t != '<PAD>' else True for t in sent]\n",
    "[is_grade.squeeze()]\n",
    "def encode(sent):\n",
    "    return [tok_to_num[t] for t in sent]\n",
    "\n",
    "def block_encode(sents):\n",
    "    return [encode(s) for s in sents]\n",
    "\n",
    "def decode(sent, drop_special=False):\n",
    "    dec = [num_to_tok[n] for n in sent]\n",
    "    if drop_special:\n",
    "        dec = [t for t in dec if t not in spec_to_num]\n",
    "    return dec\n",
    "\n",
    "def block_decode(sents, drop_special=False):\n",
    "    return [block_decode(s) for s in sents]\n",
    "\n",
    "def block_specify(sents, pad_to):\n",
    "    return [specify(sent, pad_to) for sent in sents]\n",
    "\n",
    "def block_maskify(sents):\n",
    "    return [maskify(sent) for sent in sents]\n",
    "\n",
    "def blockify(sents, pad_to):\n",
    "    X = [add_spec(sent, pad_to) for sent in sents]\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1aa75d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords_to_num = coords_map\n",
    "num_to_coords = {v:k for k,v in coords_to_num.items()}\n",
    "#\n",
    "spec_to_num = {\n",
    "    \"<CLS>\": max(num_to_coords.keys()) + 1,\n",
    "    \"<PAD>\": max(num_to_coords.keys()) + 2\n",
    "}\n",
    "num_to_spec = {v:k for k,v in spec_to_num.items()}\n",
    "#\n",
    "tok_to_num = {**spec_to_num, **coords_to_num}\n",
    "num_to_tok = {v:k for k,v in tok_to_num.items()}\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3793836",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = []\n",
    "X = []\n",
    "for p in probs:\n",
    "    grade = grade_to_num[p[\"grade\"]]\n",
    "    Y.append(grade)\n",
    "    moves = [move[\"description\"].upper() for move in p[\"moves\"]]\n",
    "    X.append(moves)\n",
    "\n",
    "dim = 15\n",
    "X = block_specify(X, dim)\n",
    "M = block_maskify(X)\n",
    "X = block_encode(X)\n",
    "\n",
    "X = np.array(X)\n",
    "M = np.array(M)\n",
    "Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e346400d",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_train = 0.85\n",
    "n_train = int(r_train * len(probs))\n",
    "#\n",
    "idcs = np.arange(len(probs))\n",
    "for _ in range(10):\n",
    "    np.random.shuffle(idcs)\n",
    "idcs_train = idcs[:n_train]\n",
    "idcs_valid = idcs[n_train:]\n",
    "#\n",
    "X_train = X[idcs_train]\n",
    "Y_train = Y[idcs_train]\n",
    "M_train = M[idcs_train]\n",
    "#\n",
    "X_valid = X[idcs_valid]\n",
    "Y_valid = Y[idcs_valid]\n",
    "M_valid = M[idcs_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24ebf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_smoother(y_true, n_classes, n_smooth):\n",
    "    y = torch.zeros(n_classes + n_smooth * 2)\n",
    "    y[y_true + n_smooth] = 1        # y\n",
    "    for smooth in range(1, n_smooth + 1):\n",
    "        y[y_true + n_smooth + smooth] = 1/(3*smooth)\n",
    "        y[y_true + n_smooth - smooth] = 1/(3*smooth)\n",
    "    return y[n_smooth:len(y) - n_smooth]\n",
    "\n",
    "\n",
    "class HoldDataset(Dataset):\n",
    "    def __init__(self, X, Y, M, n_classes=None, n_smooth=None):\n",
    "        self.M = torch.Tensor(M).bool()\n",
    "        self.X = torch.Tensor(X).long()\n",
    "        self.Y = torch.Tensor(Y).long()\n",
    "        if n_smooth is not None:\n",
    "            self.n_classes = n_classes\n",
    "            self.n_smooth = n_smooth\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.n_smooth is None:\n",
    "            return self.X[idx], self.Y[idx], self.M[idx]\n",
    "        else:\n",
    "            return self.X[idx], self.Y[idx], self.M[idx], label_smoother(self.Y[idx], self.n_classes, self.n_smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6045d7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(grade_names)\n",
    "n_smooth = 2\n",
    "\n",
    "ds_train = HoldDataset(X_train, Y_train, M_train, n_classes=n_classes, n_smooth=n_smooth)\n",
    "ds_valid = HoldDataset(X_valid, Y_valid, M_valid, n_classes=n_classes, n_smooth=n_smooth)\n",
    "#\n",
    "dl_train = DataLoader(ds_train, batch_size=256, shuffle=True, drop_last=True)\n",
    "dl_valid = DataLoader(ds_valid, batch_size=256, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11c8dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, m, ys = next(iter(dl_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64333045",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradeTransformer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, n_layers, n_classes, n_tok):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.n_layers = n_layers\n",
    "        self.n_classes = n_classes\n",
    "        self.n_tok = n_tok\n",
    "        #\n",
    "        self.embedder = nn.Embedding(n_tok, d_model)\n",
    "        #\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model = d_model,\n",
    "            nhead = n_heads,\n",
    "            batch_first=True,\n",
    "            norm_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, n_classes, bias=False),\n",
    "        )\n",
    "    def forward(self, x, src_key_padding_mask=None):\n",
    "        x = self.embedder(x)\n",
    "        x = self.encoder(x, src_key_padding_mask=src_key_padding_mask)\n",
    "        x = self.classifier(x[:, 0,:])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568be792",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y,m,ys = next(iter(dl_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c571d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 32\n",
    "n_heads = 8\n",
    "n_layers = 4\n",
    "n_classes = len(grade_to_num)\n",
    "n_tok = len(tok_to_num)\n",
    "#\n",
    "model = GradeTransformer(d_model, n_heads, n_layers, n_classes, n_tok)\n",
    "print(count_parameters(model))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08bd30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "num_epochs = 31\n",
    "model = model.to(device)\n",
    "#\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-6)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(\n",
    "        optimizer=optimizer, gamma=0.98)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "#loss_fn = torch.nn.MSELoss()\n",
    "for epoch_idx in range(num_epochs + 1):\n",
    "    #\n",
    "    model.train()\n",
    "    n_tot = 0.\n",
    "    n_cor = 0.\n",
    "    desc = \"Train [{:3}/{:3}]:\".format(epoch_idx, num_epochs)\n",
    "    pbar = tqdm(dl_train, bar_format=desc + '{bar:10}{r_bar}{bar:-10b}')\n",
    "    #\n",
    "    Y_true = []\n",
    "    Y_pred = []\n",
    "    for x, y, m, ys in pbar:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        m = m.to(device)\n",
    "        ys = ys.to(device)\n",
    "        #\n",
    "        optimizer.zero_grad()\n",
    "        out = model.forward(x, m)\n",
    "        loss = loss_fn(out, ys) \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        pbar.set_postfix(\n",
    "            {'loss': loss.item(),\n",
    "             'acc@0': soft_acuracy(y.cpu(), torch.argmax(out, dim=-1).cpu(), tol=0),\n",
    "             \"acc@1\": soft_acuracy(y.cpu(), torch.argmax(out, dim=-1).cpu(), tol=1)\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    if epoch_idx % 5 == 0 or epoch_idx == num_epochs:\n",
    "        model.eval()\n",
    "        L = 0.\n",
    "        Y_true = []\n",
    "        Y_pred = []\n",
    "        step = 0\n",
    "        for x, y, m, ys in dl_valid:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            m = m.to(device)\n",
    "            ys = ys.to(device)\n",
    "            with torch.no_grad():\n",
    "                out = model.forward(x, m)\n",
    "                loss = loss_fn(out, ys)\n",
    "                L += loss.item()\n",
    "                Y_pred.append(torch.argmax(out, dim=-1).cpu().numpy())\n",
    "                Y_true.append(y.cpu().numpy())\n",
    "                step += 1\n",
    "        Y_true = np.array(Y_true).flatten()\n",
    "        Y_pred = np.array(Y_pred).flatten()\n",
    "        acc0 = soft_acuracy(Y_true, Y_pred, tol=0)\n",
    "        acc1 = soft_acuracy(Y_true, Y_pred, tol=1)\n",
    "        print(\"  loss: {:.3f} acc@0: {:.3f} acc@1: {:.3f}\".format(L / step,acc0, acc1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a89076f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "L = 0.\n",
    "Y_true = []\n",
    "Y_pred = []\n",
    "step = 0\n",
    "for x, y, m, ys in dl_valid:\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    m = m.to(device)\n",
    "    ys = ys.to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model.forward(x, m)\n",
    "        loss = loss_fn(out, ys)\n",
    "        L += loss.item()\n",
    "        Y_pred.append(torch.argmax(out, dim=-1).cpu().numpy())\n",
    "        Y_true.append(y.cpu().numpy())\n",
    "        step += 1\n",
    "Y_true = np.array(Y_true).flatten()\n",
    "Y_pred = np.array(Y_pred).flatten()\n",
    "acc0 = soft_acuracy(Y_true, Y_pred, tol=0)\n",
    "acc1 = soft_acuracy(Y_true, Y_pred, tol=1)\n",
    "print(\"  loss: {:.3f} acc@0: {:.3f} acc@1: {:.3f}\".format(L / step,acc0, acc1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4deb38e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = Y_true\n",
    "y_pred = Y_pred\n",
    "for tol in range(2):\n",
    "    rep = soft_classification_report(y_true, y_pred, tol, target_names=grade_names, digits=3)\n",
    "    print(rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66996c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocs = torch.Tensor(list(num_to_coords.keys())).long()\n",
    "vocs = vocs.unsqueeze(0)\n",
    "#\n",
    "E = model.embedder(vocs.to(device)).detach().cpu().squeeze().numpy()\n",
    "I = vocs.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06434a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_x_tot = len(x_coords)**2\n",
    "dim_y_tot = len(y_coords)**2\n",
    "#\n",
    "dim_x = len(x_coords)\n",
    "dim_y = len(y_coords)\n",
    "\n",
    "D = np.zeros((dim_y_tot, dim_x_tot))\n",
    "\n",
    "for query_x_idx, query_x in enumerate(x_coords):\n",
    "    for query_y_idx, query_y in enumerate(y_coords):\n",
    "        #\n",
    "        query_coord = query_x + query_y\n",
    "        query_emb_idx = coords_to_num[query_coord]\n",
    "        query_emb = E[query_emb_idx]\n",
    "        #\n",
    "        for x_idx, x in enumerate(x_coords):\n",
    "            for y_idx, y in enumerate(y_coords):\n",
    "                coord = x + y\n",
    "                emb_idx = coords_to_num[coord]\n",
    "                emb = E[emb_idx]\n",
    "                #\n",
    "                #d = np.abs(np.dot(query_emb, emb) / (np.linalg.norm(query_emb) * np.linalg.norm(emb)))\n",
    "                d = np.linalg.norm(query_emb * emb)\n",
    "                #\n",
    "                d_idx_y = dim_y * query_y_idx + y_idx\n",
    "                d_idx_x = dim_x * query_x_idx + x_idx\n",
    "                \n",
    "                #if query_emb_idx == emb_idx:\n",
    "                #    d = 0\n",
    "                \n",
    "                D[d_idx_y, d_idx_x] = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9fd46f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sf = 2\n",
    "plt.figure(figsize=(dim_x * sf, dim_y * sf))\n",
    "plt.imshow(D)\n",
    "plt.xticks(list(range(5, 121, 11)), x_coords)\n",
    "plt.yticks(list(range(9, 324, 18)), y_coords)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c95fc6",
   "metadata": {},
   "source": [
    "# Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f70abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "from sklearn.manifold import TSNE\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7d0d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "L = 0.\n",
    "Y_true = []\n",
    "Y_pred = []\n",
    "R = []\n",
    "step = 0\n",
    "for x, y, m, ys in dl_valid:\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    m = m.to(device)\n",
    "    ys = ys.to(device)\n",
    "    with torch.no_grad():\n",
    "        r = model.reppresentations(x, m)\n",
    "        Y_true.append(y.cpu().numpy())\n",
    "        R.append(r[:,0,:].detach().cpu().numpy())\n",
    "        step += 1\n",
    "Y_true = np.array(Y_true).flatten()\n",
    "R = np.concatenate(R)\n",
    "#\n",
    "tsne = TSNE(n_components=3, perplexity=50)\n",
    "R = tsne.fit_transform(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b648853c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter3D(R[:,0], R[:,1], R[:,2], c=Y_true, cmap='viridis');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
