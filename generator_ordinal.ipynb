{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b0bc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c3e782",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "#\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "#\n",
    "from tqdm import tqdm\n",
    "#\n",
    "from problems import get_problems, filter_problems, filter_problems_ge\n",
    "from metrics import *\n",
    "from utils import coords_map, grade_maps, get_board_setup, draw_moves, plot_mat, count_parameters, x_coords, y_coords, draw_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb996817",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('always')  # \"error\", \"ignore\", \"always\", \"default\", \"module\" or \"once\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3514d3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "P_ROOT = Path(\"./data\")\n",
    "P_PROB = P_ROOT / \"problems\"\n",
    "P_BOARD = P_ROOT / \"boards/moonboard.png\"\n",
    "P_HOLDS = P_ROOT / \"boards/holds\"\n",
    "setup_year = 2017\n",
    "setup_angle = 45\n",
    "#\n",
    "minlen = 5\n",
    "maxlen = 12\n",
    "#\n",
    "grade_names = [\"6A+\", \"6B\", \"6B+\", \"6C\", \"6C+\", \"7A\", \"7A+\", \"7B\", \"7B+\", \"7C\", \"7C+\"]\n",
    "#grade_names = [\"6B+\", \"6C\", \"6C+\", \"7A\", \"7A+\", \"7B\", \"7B+\", \"7C\", \"7C+\", \"8A\", \"8A+\"]\n",
    "#repsge = 10\n",
    "grade_rep_ge = {\n",
    "    \"6A+\": 20,\n",
    "    \"6B\":  10,\n",
    "    \"6B+\": 5,\n",
    "    \"6C\":  5,\n",
    "    \"6C+\": 5,\n",
    "    \"7A\":5,\n",
    "    \"7A+\":5,\n",
    "    \"7B\": 5,\n",
    "    \"7B+\": 5,\n",
    "    \"7C\": 5,\n",
    "    \"7C+\": 5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c315deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = get_problems(P_PROB, setup_year, setup_angle)\n",
    "print(len(probs))\n",
    "\n",
    "probs = filter_problems_ge(probs, grade_rep_ge, grade_names, minlen, maxlen)\n",
    "print(len(probs))\n",
    "\n",
    "# GRADE MAPS\n",
    "grade_to_num, num_to_grade = grade_maps(grade_names)\n",
    "\n",
    "# COORD MAPS\n",
    "coords_to_num = coords_map\n",
    "num_to_coords = {v:k for k,v in coords_to_num.items()}\n",
    "\n",
    "# SPEC MAPS\n",
    "spec_to_num = {\n",
    "    \"[GRD]\": 0,\n",
    "    \"[HLD]\": 1,\n",
    "    \"[PAD]\": 2\n",
    "}\n",
    "num_to_spec = {v:k for k,v in spec_to_num.items()}\n",
    "\n",
    "# TOKEN MAPS\n",
    "toks = list(coords_to_num.keys()) + list(grade_to_num.keys()) + list(spec_to_num.keys())\n",
    "tok_to_num = {t:idx for idx, t in enumerate(toks)}\n",
    "num_to_tok = {v:k for k,v in tok_to_num.items()}\n",
    "#\n",
    "n_holds = len(coords_to_num)\n",
    "n_grades = len(grade_to_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f96f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "grades, nprobs = zip(*Counter(sorted([grade_to_num[p[\"grade\"]] for p in probs])).items())\n",
    "nprobs = np.array(nprobs)\n",
    "grade_freqs =  nprobs / nprobs.sum()\n",
    "class_weights = nprobs.sum() / nprobs\n",
    "\n",
    "fig, axes = plt.subplots(1, 1, figsize=(16, 8))\n",
    "plt.bar(grades, nprobs)\n",
    "#plt.yscale(\"log\")\n",
    "plt.ylabel(\"#probs\")\n",
    "plt.xlabel(\"#grades\")\n",
    "plt.xticks(grades, [num_to_grade[n] for n in grades])\n",
    "plt.show()\n",
    "#\n",
    "print(nprobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482938f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def paddify(sent, pad_to):\n",
    "    assert len(sent) > 1, \"Sentency empty\"\n",
    "    assert len(sent) <= pad_to, \"Sentences too long!, Increase padding!\"\n",
    "    return  sent + [\"[PAD]\"] * (pad_to - len(sent))\n",
    "\n",
    "def batch_paddify(sents, pad_to):\n",
    "    return [paddify(x, pad_to) for x in sents]\n",
    "\n",
    "def pad_mask(sent):\n",
    "    return [False if t != '[PAD]' else True for t in sent]\n",
    "\n",
    "def batch_pad_mask(sents):\n",
    "    return [pad_mask(sent) for sent in sents]\n",
    "\n",
    "def endecode(sent, tok_map):\n",
    "    return [tok_map[t] for t in sent]\n",
    "\n",
    "def batch_endecode(sents, tok_map):\n",
    "    return [endecode(s, tok_map) for s in sents]\n",
    "\n",
    "\n",
    "def hold_mask(sent):\n",
    "    return [True if t == \"[HLD]\" else False for t in sent]\n",
    "\n",
    "def batch_hold_mask(sents):\n",
    "    return [hold_mask(s) for s in sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5bf00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "for p in probs:\n",
    "    grade = p[\"grade\"].upper()\n",
    "    moves = [move[\"description\"].upper() for move in p[\"moves\"]]\n",
    "    X.append([grade] + moves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f957f986",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "for p in probs:\n",
    "    grade = p[\"grade\"].upper()\n",
    "    moves = [move[\"description\"].upper() for move in p[\"moves\"]]\n",
    "    X.append([grade] + moves)\n",
    "#\n",
    "X = batch_paddify(X, maxlen + 1)\n",
    "M = batch_pad_mask(X)\n",
    "#\n",
    "X = batch_endecode(X, tok_to_num)\n",
    "#\n",
    "X = np.array(X)\n",
    "M = np.array(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f57ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_train = 0.80\n",
    "n_train = int(r_train * len(probs))\n",
    "#\n",
    "idcs = np.arange(len(probs))\n",
    "for _ in range(10):\n",
    "    np.random.shuffle(idcs)\n",
    "idcs_train = idcs[:n_train]\n",
    "idcs_valid = idcs[n_train:]\n",
    "#\n",
    "X_train = X[idcs_train]\n",
    "M_train = M[idcs_train]\n",
    "#\n",
    "X_valid = X[idcs_valid]\n",
    "M_valid = M[idcs_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6b71d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_smoother(y_true, n_classes, n_smooth):\n",
    "    y = torch.zeros(n_classes + n_smooth * 2)\n",
    "    y[y_true + n_smooth] = 1        # y\n",
    "    for smooth in range(1, n_smooth + 1):\n",
    "        y[y_true + n_smooth + smooth] = 1/(2*smooth)\n",
    "        y[y_true + n_smooth - smooth] = 1/(2*smooth)\n",
    "    return y[n_smooth:len(y) - n_smooth]\n",
    "\n",
    "\n",
    "def label_ordinal(y_true, n_classes, n_smooth=0):\n",
    "    y = np.zeros(n_classes)\n",
    "    y[:y_true + 1] = 1\n",
    "    #\n",
    "    smooth_arr = 1 / (2*np.arange(1, n_smooth + 1, 1))\n",
    "    y[y_true + 1: y_true +1 +n_smooth] = smooth_arr[:n_classes - y_true - 1]\n",
    "    return y\n",
    "\n",
    "class HoldDataset(Dataset):\n",
    "    def __init__(self, X, M, transform=None):\n",
    "        self.M = M.copy()\n",
    "        self.X = X.copy()\n",
    "        #\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.transform is not None:\n",
    "            return self.transform(self.X[idx], self.M[idx])\n",
    "        else:\n",
    "            return torch.Tensor(self.X[idx]).long(), torch.Tensor(self.M[idx]).bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478c0b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(num_to_tok, tok_to_num, grade_to_num, coords_to_num, n_classes, n_smooth, p_grade=0.5):\n",
    "    def transform(x, m):\n",
    "        x = x.copy()\n",
    "        m = m.copy()\n",
    "        is_grade = np.random.rand() <= p_grade\n",
    "        if is_grade:\n",
    "            y = grade_to_num[num_to_tok[x[0]]]\n",
    "            idx = 0\n",
    "            x[0] = tok_to_num['[GRD]']\n",
    "            ys = label_ordinal(y, n_classes, n_smooth)\n",
    "        else:\n",
    "            max_idx = len(x[m == False])\n",
    "            idx = np.random.randint(1, max_idx)\n",
    "            y = coords_to_num[num_to_tok[x[idx]]]\n",
    "            x[idx] = tok_to_num['[HLD]']\n",
    "            ys = torch.zeros(n_classes)\n",
    "        \n",
    "        # Tensorify\n",
    "        y = torch.Tensor([y]).long()\n",
    "        idx = torch.Tensor([idx]).long()\n",
    "        is_grade = torch.Tensor([is_grade]).bool()\n",
    "        x = torch.Tensor(x).long()\n",
    "        m = torch.Tensor(m).bool()\n",
    "        ys = torch.Tensor(ys).float()\n",
    "        return y, ys, idx, is_grade, x, m\n",
    "\n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0e4b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_grade = 0.2\n",
    "n_smooth = 0\n",
    "batch_size = 256\n",
    "\n",
    "transform = get_transform(num_to_tok, tok_to_num, grade_to_num, coords_to_num, n_grades, n_smooth, p_grade)\n",
    "\n",
    "ds_train = HoldDataset(X_train, M_train, transform)\n",
    "ds_valid = HoldDataset(X_valid, M_valid, transform)\n",
    "#\n",
    "dl_train = DataLoader(ds_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "dl_valid = DataLoader(ds_valid, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "#\n",
    "y, ys, idx, is_grade, x, m = next(iter(dl_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba12abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_holds = get_transform(num_to_tok, tok_to_num, grade_to_num, coords_to_num, n_grades, 1, 0)\n",
    "transform_grades = get_transform(num_to_tok, tok_to_num, grade_to_num, coords_to_num, n_grades, 1, 1)\n",
    "\n",
    "ds_valid_holds = HoldDataset(X_valid, M_valid, transform_holds)\n",
    "ds_valid_grades =HoldDataset(X_valid, M_valid, transform_grades)\n",
    "#\n",
    "dl_valid_holds =  DataLoader(ds_valid_holds, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "dl_valid_grades =  DataLoader(ds_valid_grades, batch_size=batch_size, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8635c1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradeTransformer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, n_layers, n_holds, n_grades, n_tok):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.n_layers = n_layers\n",
    "        self.n_tok = n_tok\n",
    "        #\n",
    "        self.embedder = nn.Embedding(n_tok, d_model)\n",
    "        #\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model = d_model,\n",
    "            dim_feedforward=1024,\n",
    "            nhead = n_heads,\n",
    "            batch_first=True,\n",
    "            norm_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        self.classifier_holds = nn.Sequential(\n",
    "            nn.Linear(d_model, n_holds, bias=False),\n",
    "        )\n",
    "        self.classifier_grades = nn.Sequential(\n",
    "            nn.Linear(d_model, n_grades, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, src_key_padding_mask):\n",
    "        x = self.embedder(x)\n",
    "        x = self.encoder(x, src_key_padding_mask=src_key_padding_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d1928a",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 64\n",
    "n_heads = 8\n",
    "n_layers = 6\n",
    "n_tok = len(tok_to_num)\n",
    "n_holds = len(coords_to_num)\n",
    "n_grades = len(grade_to_num)\n",
    "#\n",
    "model = GradeTransformer(d_model, n_heads, n_layers, n_holds, n_grades, n_tok)\n",
    "print(count_parameters(model))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7823b42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_of_first_non_zero(x):\n",
    "    idx = torch.arange(x.shape[1], 0, -1).to(x.device)\n",
    "    x2= x * idx\n",
    "    indices = torch.argmax(x2, 1, keepdim=True)\n",
    "    return indices - 1\n",
    "\n",
    "def classify(model, y, ys, idx, is_grade, x, m):\n",
    "    out = model.forward(x, m)\n",
    "    # LOSS GRADES\n",
    "    if torch.any(is_grade):\n",
    "        y_g = y[is_grade]\n",
    "        ys_g = ys[is_grade]\n",
    "        o_g = out[is_grade]\n",
    "        idx_g = idx[is_grade]\n",
    "        o_g = o_g[torch.arange(o_g.shape[0]),idx_g,:]\n",
    "        o_g = model.classifier_grades(o_g)\n",
    "        \n",
    "        loss_g = loss_fn_g(o_g, ys_g)\n",
    "        with torch.no_grad():\n",
    "            y_g_p = index_of_first_non_zero(o_g <= 0.5).flatten()\n",
    "            acc0_g = soft_acuracy(y_g.cpu(), y_g_p.detach().cpu(), tol=0)\n",
    "            acc1_g = soft_acuracy(y_g.cpu(), y_g_p.detach().cpu(), tol=1)\n",
    "    else:\n",
    "        loss_g = torch.Tensor([0.]).to(device)\n",
    "        acc0_g = 0.\n",
    "        acc1_g = 0.\n",
    "    \n",
    "    # LOSS HOLDS\n",
    "    if torch.any(~is_grade):\n",
    "        y_h = y[~is_grade]\n",
    "        o_h = out[~is_grade]\n",
    "        idx_h = idx[~is_grade]\n",
    "        o_h = o_h[torch.arange(o_h.shape[0]),idx_h,:]\n",
    "        o_h = model.classifier_holds(o_h)\n",
    "        \n",
    "        loss_h = loss_fn_h(o_h, y_h)\n",
    "        with torch.no_grad():\n",
    "            acc0_h = soft_acuracy(y_h.cpu(), torch.argmax(o_h, dim=-1).detach().cpu(), tol=0)\n",
    "    else:\n",
    "        loss_h = torch.Tensor([0.]).to(device)\n",
    "        acc0_h = 0.\n",
    "    return loss_g, loss_h, acc0_g, acc1_g, acc0_h\n",
    "\n",
    "def get_outs(model, y, ys, idx, is_grade, x, m):\n",
    "    out = model.forward(x, m)\n",
    "    # LOSS GRADES\n",
    "    if torch.any(is_grade):\n",
    "        y_g = y[is_grade]\n",
    "        ys_g = ys[is_grade]\n",
    "        o_g = out[is_grade]\n",
    "        idx_g = idx[is_grade]\n",
    "        o_g = o_g[torch.arange(o_g.shape[0]),idx_g,:]\n",
    "        o_g = model.classifier_grades(o_g)\n",
    "    else:\n",
    "        o_g = None\n",
    "        y_g = None\n",
    "    \n",
    "    # LOSS HOLDS\n",
    "    if torch.any(~is_grade):\n",
    "        y_h = y[~is_grade]\n",
    "        o_h = out[~is_grade]\n",
    "        idx_h = idx[~is_grade]\n",
    "        o_h = o_h[torch.arange(o_h.shape[0]),idx_h,:]\n",
    "        o_h = model.classifier_holds(o_h)\n",
    "    else:\n",
    "        o_h = None\n",
    "        y_h = None\n",
    "\n",
    "    return o_g, y_g, o_h, y_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d018e34",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "num_epochs = 101\n",
    "\n",
    "model = model.to(device)\n",
    "#\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-6)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(\n",
    "        optimizer=optimizer, gamma=0.98)\n",
    "loss_fn_h = torch.nn.CrossEntropyLoss()\n",
    "loss_fn_g = torch.nn.MSELoss()\n",
    "\n",
    "epoch_acccs = []\n",
    "for epoch_idx in range(num_epochs + 1):\n",
    "    #\n",
    "    model.train()\n",
    "    desc = \"Train[{:3}/{:3}]:\".format(epoch_idx, num_epochs)\n",
    "    pbar = tqdm(dl_train, bar_format=desc + ']{r_bar}')\n",
    "    #\n",
    "    for y, ys, idx, is_grade, x, m in pbar:\n",
    "        #\n",
    "        y = y.to(device).squeeze()\n",
    "        ys = ys.to(device)\n",
    "        idx = idx.to(device).squeeze()\n",
    "        is_grade = is_grade.to(device).squeeze()\n",
    "        x = x.to(device)\n",
    "        m = m.to(device)\n",
    "        #\n",
    "        optimizer.zero_grad()\n",
    "        #\n",
    "        loss_g, loss_h, acc0_g, acc1_g, acc0_h = classify(model, y, ys, idx, is_grade, x, m)\n",
    "            \n",
    "        loss = 10 * loss_g + loss_h\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        pbar.set_postfix(\n",
    "            {'L': loss.item(),\n",
    "             'L_h': loss_h.item(),\n",
    "             'L_g': loss_g.item(),\n",
    "             'a@0': acc0_g,\n",
    "             \"a@1\": acc1_g,\n",
    "             \"ah\": acc0_h\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    if epoch_idx % 10 == 0 or epoch_idx == num_epochs:\n",
    "        model.eval()\n",
    "        ACC0G = []\n",
    "        ACC1G = []\n",
    "        ACC0H = []\n",
    "        for y, ys, idx, is_grade, x, m in dl_valid:\n",
    "            y = y.to(device).squeeze()\n",
    "            ys = ys.to(device)\n",
    "            idx = idx.to(device).squeeze()\n",
    "            is_grade = is_grade.to(device).squeeze()\n",
    "            x = x.to(device)\n",
    "            m = m.to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                _, _, acc0_g, acc1_g, acc0_h = classify(model, y, ys, idx, is_grade, x, m)\n",
    "            ACC0G.append(acc0_g)\n",
    "            ACC1G.append(acc1_g)\n",
    "            ACC0H.append(acc0_h)\n",
    "        ACC0G = np.array(ACC0G).mean()\n",
    "        ACC1G = np.array(ACC1G).mean()\n",
    "        ACC0H = np.array(ACC0H).mean()\n",
    "        epoch_acccs.append([epoch_idx, ACC0G, ACC1G, ACC0H])\n",
    "        print(\"\\tValid - a@0={:.3f} a@1={:.3f} ah={:.3f}\".format(ACC0G,ACC1G,ACC0H))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622771e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_accs = np.array(epoch_acccs)\n",
    "plt.plot(epoch_accs[:,0], epoch_accs[:,1], label=\"acc@0\")\n",
    "plt.plot(epoch_accs[:,0], epoch_accs[:,2], label=\"acc@1\")\n",
    "plt.plot(epoch_accs[:,0], epoch_accs[:,3], label=\"acc_hold\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946b6b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "#\n",
    "Y_h_t = []\n",
    "Y_h_p = []\n",
    "for _ in range(5):\n",
    "    for y, ys, idx, is_grade, x, m in dl_valid_holds:\n",
    "        y = y.to(device).squeeze()\n",
    "        ys = ys.to(device)\n",
    "        idx = idx.to(device).squeeze()\n",
    "        is_grade = is_grade.to(device).squeeze()\n",
    "        x = x.to(device)\n",
    "        m = m.to(device)\n",
    "        with torch.no_grad():\n",
    "            _, _, o_h, y_h = get_outs(model, y, ys, idx, is_grade, x, m)\n",
    "            Y_h_p.append(o_h.argmax(dim=-1).cpu().numpy())\n",
    "            Y_h_t.append(y_h.cpu().numpy())\n",
    "Y_h_t = np.concatenate(Y_h_t)\n",
    "Y_h_p = np.concatenate(Y_h_p)\n",
    "#\n",
    "rep = soft_classification_report(Y_h_t, Y_h_p, 0,\n",
    "                                 #target_names=list(coords_map.keys()),\n",
    "                                 digits=3)\n",
    "print(rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7121ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "#\n",
    "Y_g_t = []\n",
    "Y_g_p = []\n",
    "for _ in range(1):\n",
    "    for y, ys, idx, is_grade, x, m in dl_valid_grades:\n",
    "        y = y.to(device).squeeze()\n",
    "        ys = ys.to(device)\n",
    "        idx = idx.to(device).squeeze()\n",
    "        is_grade = is_grade.to(device).squeeze()\n",
    "        x = x.to(device)\n",
    "        m = m.to(device)\n",
    "        with torch.no_grad():\n",
    "            o_g, y_g, _, _ = get_outs(model, y, ys, idx, is_grade, x, m)\n",
    "            y_g_p = index_of_first_non_zero(o_g <= 0.5).flatten()\n",
    "            Y_g_p.append(y_g_p.cpu().numpy())\n",
    "            Y_g_t.append(y_g.cpu().numpy())\n",
    "Y_g_t = np.concatenate(Y_g_t)\n",
    "Y_g_p = np.concatenate(Y_g_p)\n",
    "#\n",
    "for tol in range(2):\n",
    "    if Y_g_p.min() == -1:\n",
    "        target_names =[\"-1\"] + grade_names\n",
    "    else:\n",
    "        target_names = grade_names\n",
    "    rep = soft_classification_report(Y_g_t, Y_g_p, tol,  target_names=target_names, digits=3)\n",
    "    print(rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232dd0a2",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24782309",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bac764e",
   "metadata": {},
   "source": [
    "## Grade Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78668c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_tok_nums = [tok_to_num[g] for g in grade_names]\n",
    "grade_embeddings = model.embedder(torch.Tensor(grade_tok_nums).long().unsqueeze(0).to(device)).detach().cpu().squeeze().numpy()\n",
    "grade_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a96a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "GM = np.zeros((len(grades), len(grades)))\n",
    "for i1, ge1 in enumerate(grade_embeddings):\n",
    "    for i2, ge2 in enumerate(grade_embeddings):\n",
    "        #d = 1 - np.abs(np.dot(ge1, ge2) / (np.linalg.norm(ge1) * np.linalg.norm(ge2)))\n",
    "        d = np.linalg.norm(ge1 - ge2)\n",
    "        GM[i1, i2] = d\n",
    "plot_mat(GM, col_names=grade_names, row_names=grade_names, cmap=\"Greys\", scale_factor=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1eeaf0",
   "metadata": {},
   "source": [
    "## Hold Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45799a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(num):\n",
    "    return model.embedder(torch.Tensor([num]).long().to(device)).squeeze().detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941fc013",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = list(coords_to_num.keys())\n",
    "coords_to_emb_num = {c: tok_to_num[c] for c in coords}\n",
    "coords_to_emb = {c: get_embedding(n) for c, n in coords_to_emb_num.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1434d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_x_tot = len(x_coords)**2\n",
    "dim_y_tot = len(y_coords)**2\n",
    "#\n",
    "dim_x = len(x_coords)\n",
    "dim_y = len(y_coords)\n",
    "\n",
    "D = np.zeros((dim_y_tot, dim_x_tot))\n",
    "#\n",
    "for query_x_idx, query_x in enumerate(x_coords):\n",
    "    for query_y_idx, query_y in enumerate(y_coords):\n",
    "        query_coord = query_x + query_y\n",
    "        query_emb = coords_to_emb[query_coord]\n",
    "        #\n",
    "        for x_idx, x in enumerate(x_coords):\n",
    "            for y_idx, y in enumerate(y_coords):\n",
    "                coord = x + y\n",
    "                emb = coords_to_emb[coord]\n",
    "                #\n",
    "                #d = np.abs(np.dot(query_emb, emb) / (np.linalg.norm(query_emb) * np.linalg.norm(emb)))\n",
    "                d = np.linalg.norm(query_emb - emb)\n",
    "                #d = np.abs(np.dot(query_emb, emb))\n",
    "                #\n",
    "                d_idx_y = dim_y * query_y_idx + y_idx\n",
    "                d_idx_x = dim_x * query_x_idx + x_idx\n",
    "                \n",
    "                D[d_idx_y, d_idx_x] = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6043d511",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sf = 2\n",
    "plt.figure(figsize=(dim_x * sf, dim_y * sf))\n",
    "plt.imshow(D, cmap=\"gray\")\n",
    "plt.xticks(list(range(5, 121, 11)), x_coords)\n",
    "plt.yticks(list(range(9, 324, 18)), y_coords)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4489a305",
   "metadata": {},
   "source": [
    "# Generate New Boulders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5b42ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_new_coord(grade, coords, model):\n",
    "    model.eval()\n",
    "    X_gen = [[grade] + coords + [\"[HLD]\"]]\n",
    "    X_gen = batch_paddify(X_gen, maxlen + 1)\n",
    "    M_hld = batch_hold_mask(X_gen)\n",
    "    M_pad = batch_pad_mask(X_gen)\n",
    "    X_gen = batch_endecode(X_gen, tok_to_num)\n",
    "    #\n",
    "    X_gen = np.array(X_gen)\n",
    "    M_hld = np.array(M_hld)\n",
    "    M_pad = np.array(M_pad)\n",
    "    #\n",
    "    X_gen = torch.Tensor(X_gen).long().to(device)\n",
    "    M_hld = torch.Tensor(M_hld).bool().to(device)\n",
    "    M_pad = torch.Tensor(M_pad).bool().to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out = model.forward(X_gen, M_pad)\n",
    "        out = out[M_hld]\n",
    "        out = model.classifier_holds(out)\n",
    "        out = out.argmax(dim=-1).cpu().numpy()\n",
    "        holds_pred = [num_to_coords[n] for n in out]\n",
    "    return holds_pred\n",
    "\n",
    "\n",
    "def generate_all_coord(grade, coords, model):\n",
    "    model.eval()\n",
    "    X_gen = [[grade] + coords]\n",
    "    X_gen = batch_paddify(X_gen, maxlen + 1)\n",
    "    M_hld = batch_hold_mask(X_gen)\n",
    "    M_pad = batch_pad_mask(X_gen)\n",
    "    X_gen = batch_endecode(X_gen, tok_to_num)\n",
    "    #\n",
    "    X_gen = np.array(X_gen)\n",
    "    M_hld = np.array(M_hld)\n",
    "    M_pad = np.array(M_pad)\n",
    "    #\n",
    "    X_gen = torch.Tensor(X_gen).long().to(device)\n",
    "    M_hld = torch.Tensor(M_hld).bool().to(device)\n",
    "    M_pad = torch.Tensor(M_pad).bool().to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out = model.forward(X_gen, M_pad)\n",
    "        o_h = out[M_hld]\n",
    "        o_p = out[:,0,:]\n",
    "        o_h = model.classifier_holds(o_h)\n",
    "        o_h = o_h.argmax(dim=-1).cpu().numpy()\n",
    "        holds_pred = [num_to_coords[n] for n in o_h]\n",
    "        #\n",
    "        o_g = out[:,0,:]\n",
    "        o_g = model.classifier_grades(o_g)\n",
    "        o_g = o_g.argmax(dim=-1).cpu().numpy()\n",
    "        grades_pred = [num_to_grade[n] for n in o_g]\n",
    "    return holds_pred, grades_pred\n",
    "\n",
    "def grade_boulder(coords, model, decode=True):\n",
    "    model.eval()\n",
    "    X_gen = [[\"[GRD]\"] + coords]\n",
    "    X_gen = batch_paddify(X_gen, maxlen + 1)\n",
    "    M_hld = batch_hold_mask(X_gen)\n",
    "    M_pad = batch_pad_mask(X_gen)\n",
    "    X_gen = batch_endecode(X_gen, tok_to_num)\n",
    "    #\n",
    "    X_gen = np.array(X_gen)\n",
    "    M_hld = np.array(M_hld)\n",
    "    M_pad = np.array(M_pad)\n",
    "    #\n",
    "    X_gen = torch.Tensor(X_gen).long().to(device)\n",
    "    M_hld = torch.Tensor(M_hld).bool().to(device)\n",
    "    M_pad = torch.Tensor(M_pad).bool().to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model.forward(X_gen, M_pad)\n",
    "        #\n",
    "        o_g = out[:,0,:]\n",
    "        o_g = model.classifier_grades(o_g)\n",
    "        y_g_p = index_of_first_non_zero(o_g <= 0.5)\n",
    "    grade = y_g_p.cpu().item()\n",
    "    if decode:\n",
    "        if grade == -1:\n",
    "            return \"UG\"\n",
    "        grade = num_to_grade[grade]\n",
    "    return grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495b02e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_gen = [\"F5\"]\n",
    "grade = \"6B+\"\n",
    "for _ in range(6):\n",
    "    print(X_gen)\n",
    "    H_new = generate_new_coord(grade, X_gen, model)\n",
    "    board = get_board_setup(P_BOARD, P_HOLDS, setup_year)\n",
    "    X_gen = X_gen + H_new\n",
    "    prob_board = draw_coords(board, X_gen)\n",
    "    #\n",
    "    grade_pred = grade_boulder(X_gen, model)\n",
    "    #\n",
    "    plt.figure(figsize=(5,9))\n",
    "    plt.imshow(np.array(prob_board))\n",
    "    plt.title(grade_pred)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c30bdb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_gen = [\"F5\"]\n",
    "grade = \"7B\"\n",
    "for _ in range(5):\n",
    "    print(X_gen)\n",
    "    H_new = generate_new_coord(grade, X_gen, model)\n",
    "    board = get_board_setup(P_BOARD, P_HOLDS, setup_year)\n",
    "    X_gen = X_gen + H_new\n",
    "    prob_board = draw_coords(board, X_gen)\n",
    "    #\n",
    "    grade_pred = grade_boulder(X_gen, model)\n",
    "    #\n",
    "    plt.figure(figsize=(5,9))\n",
    "    plt.imshow(np.array(prob_board))\n",
    "    plt.title(grade_pred)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e30f47",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "h_min = 5\n",
    "h_max = 9 + 1\n",
    "step = 2\n",
    "n_holds = list(range(h_min, h_max, step))\n",
    "\n",
    "n_row = len(grade_names)\n",
    "n_col = len(n_holds)\n",
    "fig,axes = plt.subplots(n_row, n_col, figsize=(n_col * 11, n_row * 18))\n",
    "\n",
    "for idx_row, grade in enumerate(grade_names):\n",
    "    for idx_col, n_hold in enumerate(n_holds):\n",
    "        X_gen = [\"[HLD]\"] * n_hold\n",
    "        G = []\n",
    "        for idx in range(len(X_gen)):\n",
    "            H_new, grades_pred = generate_all_coord(grade, X_gen, model)\n",
    "            board = get_board_setup(P_BOARD, P_HOLDS, setup_year)\n",
    "            X_gen[idx] = H_new[0]\n",
    "            grade_pred = grade_boulder(X_gen[1:], model)\n",
    "            G.append(grade_pred)\n",
    "        \n",
    "        prob_board = draw_coords(board, X_gen)\n",
    "        axes[idx_row][idx_col].imshow(np.array(prob_board))\n",
    "        axes[idx_row][idx_col].set_title(G)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20309e4f",
   "metadata": {},
   "source": [
    "# Beamsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d715640d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coords(grade, coords, model):\n",
    "    model.eval()\n",
    "    X_gen = [[grade] + coords]\n",
    "    X_gen = batch_paddify(X_gen, maxlen + 1)\n",
    "    M_hld = batch_hold_mask(X_gen)\n",
    "    M_pad = batch_pad_mask(X_gen)\n",
    "    X_gen = batch_endecode(X_gen, tok_to_num)\n",
    "    #\n",
    "    X_gen = np.array(X_gen)\n",
    "    M_hld = np.array(M_hld)\n",
    "    M_pad = np.array(M_pad)\n",
    "    #\n",
    "    X_gen = torch.Tensor(X_gen).long().to(device)\n",
    "    M_hld = torch.Tensor(M_hld).bool().to(device)\n",
    "    M_pad = torch.Tensor(M_pad).bool().to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out = model.forward(X_gen, M_pad)\n",
    "        o_h = out[M_hld]\n",
    "        o_h = model.classifier_holds(o_h)\n",
    "        #o_h = o_h.argmax(dim=-1).cpu().numpy()\n",
    "        #holds_pred = [num_to_coords[n] for n in o_h]\n",
    "        #\n",
    "        #o_g = out[:,0,:]\n",
    "        #o_g = model.classifier_grades(o_g)\n",
    "        #o_g = o_g.argmax(dim=-1).cpu().numpy()\n",
    "        #grades_pred = [num_to_grade[n] for n in o_g]\n",
    "    return o_h\n",
    "\n",
    "def sequence_probabililty(seq):\n",
    "    seq = np.array(seq)\n",
    "    return np.log(seq).mean()\n",
    "\n",
    "def all_seqs_beamsearch(seq_len, grade, top_k, n_beams):\n",
    "    all_seqs = [[\"[HLD]\"] * seq_len]\n",
    "    all_probs = [[1.]]\n",
    "    #\n",
    "    for hold_idx in range(seq_len):\n",
    "        #\n",
    "        new_probs = []\n",
    "        new_seqs = []\n",
    "        #\n",
    "        for seq_idx in range(len(all_seqs)):\n",
    "            #\n",
    "            seq = all_seqs[seq_idx][::]\n",
    "            seq_prob = all_probs[seq_idx][::]\n",
    "            #\n",
    "            #print(hold_idx, seq_idx, seq)\n",
    "            #\n",
    "            o_h = get_coords(grade, seq, model)[0]\n",
    "            p_h = torch.softmax(o_h, dim=-1)\n",
    "            #\n",
    "            top_probs, top_idcs = torch.topk(p_h, top_k)\n",
    "            top_probs = top_probs.cpu().numpy()\n",
    "            top_idcs = top_idcs.cpu().numpy()\n",
    "            top_holds = [num_to_tok[idx] for idx in top_idcs]\n",
    "            #\n",
    "            for new_hold, new_prob in zip(top_holds, top_probs):\n",
    "                seq_new = seq[::]\n",
    "                seq_new[hold_idx] = new_hold\n",
    "                #import pdb\n",
    "                #pdb.set_trace()\n",
    "                #\n",
    "                seq_new_prob = seq_prob[::]\n",
    "                seq_new_prob.append(new_prob)\n",
    "\n",
    "                new_probs.append(seq_new_prob)\n",
    "                new_seqs.append(seq_new)\n",
    "\n",
    "        new_seqs_probs = [sequence_probabililty(seq) for seq in new_probs]\n",
    "        new_idcs = np.argsort(new_seqs_probs)[-n_beams:]\n",
    "        #\n",
    "        all_probs = [new_probs[i] for i in new_idcs]\n",
    "        all_seqs = [new_seqs[i] for i in new_idcs]\n",
    "    return all_probs, all_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556d9d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 6\n",
    "grade = \"7C\"\n",
    "top_k = 100\n",
    "n_beams = 300\n",
    "\n",
    "all_seqs = [[\"[HLD]\"] * seq_len]\n",
    "all_probs = [[1.]]\n",
    "#\n",
    "for hold_idx in range(seq_len):\n",
    "    #\n",
    "    new_probs = []\n",
    "    new_seqs = []\n",
    "    #\n",
    "    for seq_idx in range(len(all_seqs)):\n",
    "        #\n",
    "        seq = all_seqs[seq_idx][::]\n",
    "        seq_prob = all_probs[seq_idx][::]\n",
    "        #\n",
    "        #print(hold_idx, seq_idx, seq)\n",
    "        #64\n",
    "        o_h = get_coords(grade, seq, model)[0]\n",
    "        p_h = torch.softmax(o_h, dim=-1)\n",
    "        #\n",
    "        top_probs, top_idcs = torch.topk(p_h, top_k)\n",
    "        top_probs = top_probs.cpu().numpy()\n",
    "        top_idcs = top_idcs.cpu().numpy()\n",
    "        top_holds = [num_to_tok[idx] for idx in top_idcs]\n",
    "        #\n",
    "        for new_hold, new_prob in zip(top_holds, top_probs):\n",
    "            seq_new = seq[::]\n",
    "            seq_new[hold_idx] = new_hold\n",
    "            #import pdb\n",
    "            #pdb.set_trace()\n",
    "            #\n",
    "            seq_new_prob = seq_prob[::]\n",
    "            seq_new_prob.append(new_prob)\n",
    "\n",
    "            new_probs.append(seq_new_prob)\n",
    "            new_seqs.append(seq_new)\n",
    "    \n",
    "    new_seqs_probs = [sequence_probabililty(seq) for seq in new_probs]\n",
    "    new_idcs = np.argsort(new_seqs_probs)[-n_beams:]\n",
    "    #\n",
    "    all_probs = [new_probs[i] for i in new_idcs]\n",
    "    all_seqs = [new_seqs[i] for i in new_idcs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365a1f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_seqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe28278",
   "metadata": {},
   "source": [
    "# Generate\n",
    "- 6a+ 6,7,8,\n",
    "- 6b\n",
    "- 6b+\n",
    "- 6c\n",
    "- 6c+\n",
    "- 7a\n",
    "- 7a+\n",
    "- 7b\n",
    "- 7b+\n",
    "- 7c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c123f2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEN_BOULDERS = [\n",
    "    (\"6A+\", 6, 2),\n",
    "    (\"6A+\", 7, 2),\n",
    "    (\"6A+\", 8, 2),\n",
    "    (\"6A+\", 9, 2),\n",
    "    #\n",
    "    (\"6B\", 6, 2),\n",
    "    (\"6B\", 7, 2),\n",
    "    #\n",
    "    (\"6B+\", 6, 2),\n",
    "    (\"6B+\", 7, 2),\n",
    "    #\n",
    "    (\"6C\", 5, 2),\n",
    "    (\"6C\", 6, 2),\n",
    "    (\"6C\", 7, 2),\n",
    "    #\n",
    "    (\"7A\", 5, 2),\n",
    "    (\"7A\", 6, 2),\n",
    "     #\n",
    "    (\"7A+\", 5, 2),\n",
    "    (\"7A+\", 6, 2),\n",
    "    #\n",
    "    (\"7B\", 5, 2),\n",
    "    (\"7B\", 6, 2),  \n",
    "    #\n",
    "    (\"7B+\", 5, 2),\n",
    "    (\"7B+\", 6, 2), \n",
    "    #\n",
    "    (\"7C\", 5, 2),\n",
    "    (\"7C\", 6, 2), \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d284d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 100\n",
    "n_beams = 500\n",
    "GEN_ALL = []\n",
    "for grade, seq_len, n_tot in GEN_BOULDERS:\n",
    "    print(grade, seq_len)\n",
    "    all_probs, all_seqs = all_seqs_beamsearch(seq_len, grade, top_k, n_beams)\n",
    "    #\n",
    "    all_seqs = set([tuple(sorted(s)) for s in all_seqs])\n",
    "    all_seqs = [list(s) for s in all_seqs]\n",
    "    #\n",
    "    seqs = []\n",
    "    for seq in all_seqs:\n",
    "        grade_pred = grade_boulder(seq, model)\n",
    "        if grade_pred == \"UG\":\n",
    "            continue\n",
    "        if grade_to_num[grade] == grade_to_num[grade_pred] or grade_to_num[grade] == grade_to_num[grade_pred] - 1:\n",
    "            seqs.append(seq)\n",
    "    #\n",
    "    for idx in np.random.randint(0, len(seqs), size=min(n_tot, len(seqs))):\n",
    "        seq = seqs[idx]\n",
    "        GEN_ALL.append((grade, seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971b15f8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for grade, seq in GEN_ALL:\n",
    "    prob_board = draw_coords(board, seq)\n",
    "    prob_board = draw_coords(board, seq)\n",
    "    plt.figure(figsize=(5,9))\n",
    "    plt.imshow(np.array(prob_board))\n",
    "    plt.title(grade)\n",
    "    plt.show()   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40c2d5e",
   "metadata": {},
   "source": [
    "# Gen One Boulder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20bfdce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_tot = 5\n",
    "seq_len = 7\n",
    "grade = \"7A\"\n",
    "top_k = 100\n",
    "n_beams = 500\n",
    "all_probs, all_seqs = all_seqs_beamsearch(seq_len, grade, top_k, n_beams)\n",
    "#\n",
    "all_seqs = set([tuple(sorted(s)) for s in all_seqs])\n",
    "all_seqs = [list(s) for s in all_seqs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5bc041",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "seqs = []\n",
    "for seq in all_seqs:\n",
    "    grade_pred = grade_boulder(seq, model)\n",
    "    if grade_to_num[grade] <= grade_to_num[grade_pred]:\n",
    "        seqs.append(seq)\n",
    "#\n",
    "for idx in np.random.randint(0, len(seqs), size=min(n_tot, len(seqs))):\n",
    "    seq = seqs[idx]\n",
    "    prob_board = draw_coords(board, seq)\n",
    "    prob_board = draw_coords(board, seq)\n",
    "    plt.figure(figsize=(5,9))\n",
    "    plt.imshow(np.array(prob_board))\n",
    "    plt.title(\"\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44158f8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
